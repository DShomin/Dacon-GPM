{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Add, BatchNormalization, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "train_files = os.listdir('../data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for file in train_files:\n",
    "    try:\n",
    "        data = np.load('../data/train/'+file).astype('float32')\n",
    "        train.append(data)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "test = []\n",
    "for sub_id in submission['id']:\n",
    "    data = np.load('../data/test/'+'subset_'+sub_id+'.npy').astype('float32')\n",
    "    test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[:,:,:,:10]\n",
    "y_train = train[:,:,:,14]\n",
    "# test = test[:,:,:,:10]\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74436, 40, 40, 10),\n",
       " (74436, 40, 40, 1),\n",
       " (1909, 40, 40, 10),\n",
       " (1909, 40, 40, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.025, random_state=7777)\n",
    "y_train = np.expand_dims(y_train, axis=-1)\n",
    "y_test = np.expand_dims(y_test, axis=-1)\n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_ = y_train.reshape(-1,y_train.shape[1]*y_train.shape[2])\n",
    "\n",
    "# x_train = np.delete(x_train, np.where(y_train_<0)[0], axis=0)\n",
    "# y_train = np.delete(y_train, np.where(y_train_<0)[0], axis=0)\n",
    "# y_train = y_train.reshape(-1, x_train.shape[1], x_train.shape[2],1)\n",
    "# y_test = y_test.reshape(-1, y_test.shape[1], y_test.shape[2],1)\n",
    "\n",
    "# y_train_ = np.delete(y_train_, np.where(y_train_<0)[0], axis=0)\n",
    "\n",
    "# x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train[np.sum((y_train_>= 50))]\n",
    "# y_train = y_train[np.sum((y_train_>= 50))]\n",
    "\n",
    "# x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(x_train, y_train):\n",
    "    rotate_X_90 = np.zeros_like(x_train)\n",
    "    rotate_Y_90 = np.zeros_like(y_train)\n",
    "\n",
    "    for j in range(rotate_X_90.shape[0]):\n",
    "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
    "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
    "        for i in range(10):\n",
    "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
    "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
    "\n",
    "        rotate_X_90[j,:,:,:] = rotate_x\n",
    "        rotate_Y_90[j,:,:,:] = rotate_y\n",
    "\n",
    "    rotate_X_180 = np.zeros_like(x_train)\n",
    "    rotate_Y_180 = np.zeros_like(y_train)\n",
    "\n",
    "    for j in range(rotate_X_180.shape[0]):\n",
    "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
    "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
    "        for i in range(10):\n",
    "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
    "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
    "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
    "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
    "\n",
    "        rotate_X_180[j,:,:,:] = rotate_x\n",
    "        rotate_Y_180[j,:,:,:] = rotate_y\n",
    "\n",
    "    rotate_X_270 = np.zeros_like(x_train)\n",
    "    rotate_Y_270 = np.zeros_like(y_train)\n",
    "\n",
    "    for j in range(rotate_X_270.shape[0]):\n",
    "        rotate_x=np.zeros([x_train.shape[1],x_train.shape[2],10])\n",
    "        rotate_y=np.zeros([x_train.shape[1],x_train.shape[2],1])\n",
    "        for i in range(10):\n",
    "            rotate_x[:,:,i]=np.rot90(x_train[j,:,:,i])\n",
    "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
    "            rotate_x[:,:,i]=np.rot90(rotate_x[:,:,i])\n",
    "        rotate_y[:,:,0]=np.rot90(y_train[j,:,:,0])\n",
    "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
    "        rotate_y[:,:,0]=np.rot90(rotate_y[:,:,0])\n",
    "\n",
    "        rotate_X_270[j,:,:,:] = rotate_x\n",
    "        rotate_Y_270[j,:,:,:] = rotate_y\n",
    "\n",
    "    x_train = np.concatenate((x_train, rotate_X_90, rotate_X_180, rotate_X_270), axis = 0)\n",
    "    y_train = np.concatenate((y_train, rotate_Y_90, rotate_Y_180, rotate_Y_270), axis = 0)\n",
    "    del rotate_X_90, rotate_X_180, rotate_X_270\n",
    "\n",
    "    x_T = np.zeros_like(x_train)\n",
    "    y_T = np.zeros_like(y_train)\n",
    "\n",
    "    for i in range(x_train.shape[0]):\n",
    "        for j in range(x_train.shape[3]):\n",
    "            x_T[i,:,:,j] = x_train[i,:,:,j].T\n",
    "        y_T[i,:,:,0] = y_train[i,:,:,0].T\n",
    "\n",
    "    x_train = np.concatenate((x_train, x_T), axis = 0)\n",
    "    y_train = np.concatenate((y_train, y_T), axis = 0)\n",
    "\n",
    "    del x_T,y_T\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_over_fscore(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: sample_submission.csv 형태의 실제 값\n",
    "    y_pred: sample_submission.csv 형태의 예측 값\n",
    "    '''\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(1, -1)[0]  \n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    # 실제값이 0.1 이상인 픽셀의 위치 확인\n",
    "    IsGreaterThanEqualTo_PointOne = y_true >= 0.1\n",
    "    \n",
    "    # 실제 값에 결측값이 없는 픽셀의 위치 확인 \n",
    "    IsNotMissing = y_true >= 0\n",
    "    \n",
    "    # mae 계산\n",
    "    mae = np.mean(np.abs(y_true[IsGreaterThanEqualTo_PointOne] - y_pred[IsGreaterThanEqualTo_PointOne]))\n",
    "    \n",
    "    # f1_score 계산 위해, 실제값에 결측값이 없는 픽셀에 대해 1과 0으로 값 변환\n",
    "    y_true = np.where(y_true[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[IsNotMissing] >= 0.1, 1, 0)\n",
    "    \n",
    "    # f1_score 계산    \n",
    "    f_score = f1_score(y_true, y_pred) \n",
    "    # f1_score가 0일 나올 경우를 대비하여 소량의 값 (1e-07) 추가 \n",
    "    return mae / (f_score + 1e-07) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "\n",
    "def fscore_keras(y_true, y_pred):\n",
    "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float16, name='fscore_keras')\n",
    "    return score\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float16,  name='custom_mse') \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs=Input(x_train.shape[1:])\n",
    "    \n",
    "    bn=BatchNormalization()(inputs)\n",
    "    conv0=Conv2D(256, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
    "    \n",
    "    bn=BatchNormalization()(conv0)\n",
    "    conv=Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    concat=concatenate([conv0, conv], axis=3)\n",
    "    \n",
    "    bn=BatchNormalization()(concat)\n",
    "    conv=Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
    "    concat=concatenate([concat, conv], axis=3)\n",
    "        \n",
    "    for i in range(5):\n",
    "        bn=BatchNormalization()(concat)\n",
    "        conv=Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(bn)\n",
    "        concat=concatenate([concat, conv], axis=3)\n",
    "    \n",
    "    bn=BatchNormalization()(concat)\n",
    "    outputs=Conv2D(1, kernel_size=1, strides=1, padding='same', activation='relu')(bn)\n",
    "    \n",
    "    model=Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    \n",
    "    return 1 - (numerator + 1) / (denominator + 1)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    mae_loss = losses.mean_absolute_error(y_true, y_pred)\n",
    "    y_true, y_pred = tf.math.sigmoid(y_true), tf.math.sigmoid(y_pred)\n",
    "    \n",
    "    return losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) + mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "bs = 64\n",
    "def train_model(x_data, y_data, k, s):\n",
    "    \n",
    "    k_fold = KFold(n_splits=k, shuffle=True, random_state=7777)\n",
    "    sp = [x for x in k_fold.split(x_data)]\n",
    "    model_number = 0\n",
    "    train_idx, val_idx = sp[s]\n",
    "#     for train_idx, val_idx in sp[s]:\n",
    "    x_train, y_train = x_data[train_idx], y_data[train_idx]\n",
    "    x_val, y_val = x_data[val_idx], y_data[val_idx]\n",
    "\n",
    "# 데이터를 부풀릴시 많은 양의 메모리가 필요\n",
    "#             x_train, y_train = data_generator(x_train, y_train)\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "#             data_gen_args = dict(rotation_range=180)\n",
    "#             image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "#             mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "#             image_generator = image_datagen.flow(x_train, seed=42, batch_size=bs, shuffle=True)\n",
    "#             mask_generator = mask_datagen.flow(y_train, seed=42, batch_size=bs, shuffle=True)\n",
    "#             train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "#             opt = tfa.optimizers.SWA(opt, start_averaging=30, average_period=5)\n",
    "    model.compile(loss=bce_dice_loss, optimizer=opt, metrics=[score])\n",
    "\n",
    "    callbacks_list = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            factor=0.8\n",
    "        ),\n",
    "\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = '../models/model'+str(s)+'.h5',\n",
    "            monitor='val_score',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "#                 SWA(start_epoch=30, \n",
    "#                   lr_schedule='constant', \n",
    "#                   swa_lr=0.001, \n",
    "#                   verbose=1)\n",
    "    ]\n",
    "\n",
    "#             model.fit_generator(train_generator, steps_per_epoch=(len(y_train) // bs), epochs=50, validation_data=(x_val, y_val), callbacks=callbacks_list)\n",
    "    model.fit(x_train, y_train, batch_size=bs, epochs=50, validation_data=(x_val, y_val), callbacks=callbacks_list)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "931/931 [==============================] - 441s 473ms/step - loss: 46.2286 - score: 2.4363 - val_loss: 43.4980 - val_score: 2.0599 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "767/931 [=======================>......] - ETA: 1:24 - loss: 44.4333 - score: 2.0799"
     ]
    },
    {
     "ename": "_NotOkStatusException",
     "evalue": "InvalidArgumentError: Error while reading CompositeTensor._type_spec.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_NotOkStatusException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b3eedfa069e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-77f86860dbeb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(x_data, y_data, k, s)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#             model.fit_generator(train_generator, steps_per_epoch=(len(y_train) // bs), epochs=50, validation_data=(x_val, y_val), callbacks=callbacks_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2735\u001b[0m           *args, **kwargs)\n\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   2573\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m       input_signature = pywrap_tfe.TFE_Py_EncodeArg(inputs,\n\u001b[0;32m-> 2575\u001b[0;31m                                                     include_tensor_ranks_only)\n\u001b[0m\u001b[1;32m   2576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_NotOkStatusException\u001b[0m: InvalidArgumentError: Error while reading CompositeTensor._type_spec."
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "models = []\n",
    "\n",
    "train_model(x_train, y_train, k=k, s=0)\n",
    "\n",
    "for n in range(k):\n",
    "    model = load_model('models/model'+str(n)+'.h5', custom_objects = {'score':score,'fscore_keras':fscore_keras})\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(x_test))\n",
    "    print(mae_over_fscore(y_test, preds[-1]))\n",
    "\n",
    "pred = sum(preds)/len(preds)\n",
    "print(mae_over_fscore(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models:\n",
    "    preds.append(model.predict(test))\n",
    "\n",
    "pred = sum(preds)/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1:] = pred.reshape(-1,1600)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
